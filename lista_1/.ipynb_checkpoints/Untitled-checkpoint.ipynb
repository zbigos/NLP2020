{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LISTA 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOBAL SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZADANIE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SJP already downloaded, unpacking\n",
      "read 3045880 words from SJP, longest one is 15 letters long!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"sjp-20201007.zip\"):\n",
    "    print(\"doing a succ on SJP dictionary\")\n",
    "    os.system(\"wget https://sjp.pl/slownik/growy/sjp-20201007.zip\")\n",
    "else:\n",
    "    print(\"SJP already downloaded, unpacking\")\n",
    "\n",
    "if not os.path.isfile(\"slowa.txt\"):\n",
    "    os.system(\"unzip sjp-20201007.zip\")\n",
    "\n",
    "words = []\n",
    "with open(\"slowa.txt\", \"r\") as f:\n",
    "    words = set(f.read().split(\"\\n\"))\n",
    "    \n",
    "# the dict is mentally challenged and does not have 'i' in it\n",
    "words.add(\"i\")\n",
    "words.add(\"z\")\n",
    "\n",
    "\n",
    "longest = max(len(w) for w in words)    \n",
    "\n",
    "print(f\"read {len(words)} words from SJP, longest one is {longest} letters long!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kamienie', 'testowanie']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MaxMatch(w, verbose = False):\n",
    "    tokenized = []\n",
    "    succ = 1\n",
    "    best = (0, \"reeee\")\n",
    "    \n",
    "    while len(w) > 0:\n",
    "        try_ = w[:succ]\n",
    "        if try_ in words:\n",
    "            if verbose:\n",
    "                print(f\"found candidate {try_}\")\n",
    "            best = (succ, try_) \n",
    "\n",
    "        succ += 1\n",
    "        \n",
    "        if succ > longest or try_ == w:\n",
    "            if best[0] == 0:\n",
    "                raise Exception(f\"could not match {w[:succ]}\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"longest match was {best[1]}\")\n",
    "                tokenized.append(best[1])\n",
    "                if verbose:\n",
    "                    print(f\"transforming {w} -> {w[best[0]:]}\")\n",
    "                w = w[best[0]:]\n",
    "                succ = 1\n",
    "                best = (0, \"reeee\")\n",
    "    return tokenized\n",
    "                \n",
    "MaxMatch(\"kamienietestowanie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZADANIE 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3889/3889 [00:00<00:00, 18309.14it/s]\n",
      "  1%|          | 418/39566 [00:00<00:09, 4169.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 39566 snippets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39566/39566 [00:11<00:00, 3583.79it/s]\n",
      "100%|██████████| 662051/662051 [00:00<00:00, 2255949.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_bigrams():\n",
    "    text = []\n",
    "    for dir in tqdm(os.listdir(\"corpora/\")):\n",
    "        with open(f\"corpora/{dir}/text.xml\", \"r\") as corpfile:\n",
    "            corpseg = corpfile.read().split(\"\\n\")\n",
    "            for l in corpseg:\n",
    "                if \"<ab\" in l:\n",
    "                    l = l.replace(\"</ab>\", \"\")\n",
    "                    l = l.split(\">\")[1]\n",
    "                    text.append(l.lower())\n",
    "    print(f\"got {len(text)} snippets\")\n",
    "    \n",
    "    import nltk\n",
    "    import collections\n",
    "    \n",
    "    bigram_stat = collections.defaultdict(lambda: 0)\n",
    "    for sentence in tqdm(text):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        for k in range(len(tokens) - 1):\n",
    "            key = f\"{tokens[k]}#{tokens[k+1]}\"\n",
    "            bigram_stat[key] += 1\n",
    "            \n",
    "    baked_bigrams = [(bigram_stat[k], k) for k in bigram_stat.keys()]\n",
    "    baked_bigrams = sorted(baked_bigrams, reverse=True)\n",
    "    return baked_bigrams\n",
    "    \n",
    "bigrams = calculate_bigrams()\n",
    "writeable = [f\"{a[0]}#{a[1]}\" for a in bigrams]\n",
    "f = open(\"bigrams.txt\", \"a\")\n",
    "\n",
    "for l in tqdm(writeable):\n",
    "    f.write(l + \"\\n\")\n",
    "f.close()\n",
    "os.system(\"sync\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
